# Benchmarking Guide

Esta gu√≠a te ayudar√° a dimensionar el tiempo de entrenamiento y los recursos necesarios para entrenar el modelo MTL Transformer en tu sistema.

## ¬øPor qu√© hacer benchmarking?

Antes de ejecutar entrenamientos largos (50-200 √©pocas), es importante saber:
- ‚è±Ô∏è **Cu√°nto tiempo** tomar√° el entrenamiento completo
- üíª **Cu√°ntos recursos** consumir√° (GPU, RAM, CPU)
- ‚öôÔ∏è **Qu√© configuraci√≥n** (batch_size) es √≥ptima para tu hardware
- üö® **Si tu sistema** puede manejar el dataset completo

## Uso R√°pido

### 1. Benchmark B√°sico (Recomendado para empezar)

```bash
docker compose run --rm trainer-gpu python -m src.benchmark \
  --parquet data/processed/bdg2_cleaned.parquet \
  --output_dir data/benchmark
```

**Qu√© hace:**
- Ejecuta 3 √©pocas de entrenamiento
- Usa dataset limitado (150 buildings de entrenamiento)
- Monitorea uso de CPU, RAM, GPU
- Genera reporte con estimaciones de tiempo

**Tiempo estimado:** 5-15 minutos dependiendo de tu GPU

### 2. Benchmark con Dataset Completo

```bash
docker compose run --rm trainer-gpu python -m src.benchmark \
  --parquet data/processed/bdg2_cleaned.parquet \
  --output_dir data/benchmark \
  --use_full_dataset
```

**Qu√© hace:**
- Ejecuta 3 √©pocas con TODOS los buildings (1,578)
- Requiere al menos 6GB de RAM
- Proporciona estimaciones m√°s precisas para producci√≥n

**Tiempo estimado:** 15-30 minutos dependiendo de tu GPU

### 3. Benchmark Personalizado

```bash
docker compose run --rm trainer-gpu python -m src.benchmark \
  --parquet data/processed/bdg2_cleaned.parquet \
  --benchmark_epochs 5 \
  --batch_size 64 \
  --output_dir data/benchmark \
  --use_full_dataset
```

**Par√°metros:**
- `--benchmark_epochs`: M√°s √©pocas = estimaciones m√°s precisas (pero m√°s tiempo)
- `--batch_size`: Prueba diferentes tama√±os para encontrar el √≥ptimo
- `--config`: Usa un archivo de configuraci√≥n personalizado

## Interpretando los Resultados

### Reporte de Consola

Al finalizar, ver√°s un reporte como este:

```
================================================================================
                         BENCHMARK REPORT
================================================================================

üìä SYSTEM INFORMATION
--------------------------------------------------------------------------------
  TensorFlow Version:    2.15.0
  CUDA Available:        True
  GPU Count:             1
  GPU Model:             NVIDIA GeForce RTX 3060
  GPU Memory:            12288 MB

‚öôÔ∏è  TRAINING CONFIGURATION
--------------------------------------------------------------------------------
  Batch Size:            32
  Benchmark Epochs:      3
  Mixed Precision:       True
  Dataset Mode:          full

üíª RESOURCE USAGE
--------------------------------------------------------------------------------
  CPU Usage:             avg=45.2%, max=78.1%
  Memory Usage:          avg=52.3%, max=68.7%
  GPU Usage:             avg=92.5%, max=98.2%
  GPU Memory:            avg=75.3%, max=85.1%
  GPU Temperature:       avg=68.5¬∞C, max=72.0¬∞C

‚è±Ô∏è  TIMING STATISTICS
--------------------------------------------------------------------------------
  Avg Epoch Time:        0.90 min (54.2s)
  Std Epoch Time:        ¬±2.3s
  Avg Batch Time:        0.125s

üìà TIME ESTIMATES
--------------------------------------------------------------------------------

  For 50 epochs:
    Full training:       0.75 hours (45.0 min)
    With early stop:     0.53 hours (31.5 min) [~35 epochs]

  For 100 epochs:
    Full training:       1.50 hours (90.0 min)
    With early stop:     1.05 hours (63.0 min) [~70 epochs]

  For 200 epochs:
    Full training:       3.00 hours (180.0 min)
    With early stop:     2.10 hours (126.0 min) [~140 epochs]

üí° RECOMMENDATIONS
--------------------------------------------------------------------------------

  1. GPU memory usage is optimal (75.3%)
     ‚Üí Keep current batch_size=32

================================================================================
```

### Archivo JSON

Adem√°s del reporte de consola, se guarda un archivo JSON con todos los detalles:

```bash
data/benchmark/benchmark_results_<timestamp>.json
```

Este archivo contiene:
- Tiempos de cada √©poca individual
- Estad√≠sticas detalladas de cada batch
- Uso de recursos a lo largo del tiempo
- Recomendaciones y configuraci√≥n usada

## Recomendaciones Seg√∫n Resultados

### Si GPU Memory < 50%

```
üí° Low GPU memory usage (<50%)
   ‚Üí Increase batch_size to 64 for faster training
```

**Acci√≥n:** Aumenta el batch_size para acelerar el entrenamiento:

```bash
# Prueba con batch_size mayor
docker compose run --rm trainer-gpu python -m src.benchmark \
  --parquet data/processed/bdg2_cleaned.parquet \
  --batch_size 64
```

### Si GPU Memory > 90%

```
üí° High GPU memory usage (>90%)
   ‚Üí Decrease batch_size to 16 to avoid OOM errors
```

**Acci√≥n:** Reduce el batch_size para evitar errores de memoria:

```bash
# Prueba con batch_size menor
docker compose run --rm trainer-gpu python -m src.benchmark \
  --parquet data/processed/bdg2_cleaned.parquet \
  --batch_size 16
```

### Si GPU Memory 50-90%

```
üí° GPU memory usage is optimal (75%)
   ‚Üí Keep current batch_size=32
```

**Acci√≥n:** Tu configuraci√≥n es √≥ptima, procede con el entrenamiento.

## Ejemplos de Hardware

### RTX 3060 (12GB VRAM)

```
Dataset completo:
  Batch Size:     32
  √âpoca:          ~54 segundos
  50 √©pocas:      ~45 minutos (31 min con early stop)
  GPU Memory:     ~75%
  Recomendaci√≥n:  √ìptimo, usar batch_size=32
```

### RTX 3050 (8GB VRAM)

```
Dataset completo:
  Batch Size:     16 (reducido para evitar OOM)
  √âpoca:          ~78 segundos
  50 √©pocas:      ~65 minutos (45 min con early stop)
  GPU Memory:     ~85%
  Recomendaci√≥n:  Usar batch_size=16, considerar dataset limitado
```

### GTX 1660 Ti (6GB VRAM)

```
Dataset limitado (recomendado):
  Batch Size:     16
  √âpoca:          ~45 segundos
  50 √©pocas:      ~37 minutos (26 min con early stop)
  GPU Memory:     ~88%
  Recomendaci√≥n:  Usar dataset limitado, batch_size=16
```

## Flujo de Trabajo Recomendado

### 1. Primera vez (sin datos hist√≥ricos)

```bash
# Paso 1: Benchmark b√°sico para conocer tu hardware
docker compose run --rm trainer-gpu python -m src.benchmark \
  --parquet data/processed/bdg2_cleaned.parquet

# Paso 2: Revisa el reporte y ajusta batch_size si es necesario

# Paso 3: Benchmark con dataset completo si tu sistema lo soporta
docker compose run --rm trainer-gpu python -m src.benchmark \
  --parquet data/processed/bdg2_cleaned.parquet \
  --use_full_dataset

# Paso 4: Usa las estimaciones para planear tu entrenamiento
```

### 2. Experimentando con configuraciones

```bash
# Prueba diferentes batch sizes
for bs in 16 32 64; do
  docker compose run --rm trainer-gpu python -m src.benchmark \
    --parquet data/processed/bdg2_cleaned.parquet \
    --batch_size $bs \
    --output_dir data/benchmark
done

# Compara los resultados en data/benchmark/*.json
```

### 3. Antes de entrenamientos largos

```bash
# Antes de 100-200 √©pocas, haz benchmark de 5 √©pocas
docker compose run --rm trainer-gpu python -m src.benchmark \
  --parquet data/processed/bdg2_cleaned.parquet \
  --benchmark_epochs 5 \
  --use_full_dataset

# Las estimaciones ser√°n m√°s precisas con m√°s √©pocas
```

## Troubleshooting

### Error: Out of Memory (OOM)

```
ResourceExhaustedError: OOM when allocating tensor
```

**Soluci√≥n:**
1. Reduce batch_size a 16 o 8
2. Usa dataset limitado (sin `--use_full_dataset`)
3. Considera usar solo CPU si tu GPU es muy peque√±a

### Benchmark muy lento

Si el benchmark tarda mucho:
- Reduce `--benchmark_epochs` a 2
- Usa dataset limitado para pruebas iniciales
- Verifica que est√©s usando GPU (no CPU)

### No se detecta GPU

```
‚ö†Ô∏è  No GPU detected - will run on CPU
```

**Soluci√≥n:**
1. Verifica que nvidia-docker est√© instalado
2. Ejecuta `nvidia-smi` para verificar que la GPU est√© disponible
3. Reconstruye el contenedor: `docker compose build --no-cache trainer-gpu`

## Preguntas Frecuentes

### ¬øCu√°ntas √©pocas debo usar para el benchmark?

- **2-3 √©pocas**: Suficiente para estimaci√≥n r√°pida (5-15 min)
- **5 √©pocas**: Mejor precisi√≥n, recomendado antes de entrenamientos largos (15-30 min)
- **10 √©pocas**: Muy preciso, solo si tienes tiempo

### ¬øDebo usar dataset completo para el benchmark?

- **S√≠** si planeas entrenar con dataset completo en producci√≥n
- **No** si solo est√°s probando el pipeline o tu hardware es limitado (<6GB RAM)

### ¬øLos resultados son exactos?

Los resultados son **estimaciones** basadas en:
- Promedio de √©pocas del benchmark
- Suposici√≥n de 70% de √©pocas completadas (early stopping)
- Condiciones actuales del sistema

Las estimaciones son generalmente precisas ¬±10-15%.

### ¬øPuedo comparar diferentes GPUs?

S√≠, ejecuta el mismo comando en diferentes m√°quinas y compara los archivos JSON generados.

## Siguiente Paso

Una vez que tengas las estimaciones de tu benchmark, procede con el entrenamiento completo:

```bash
# Usar la configuraci√≥n √≥ptima del benchmark
docker compose run --rm trainer-gpu python -m src.main \
  --parquet data/processed/bdg2_cleaned.parquet \
  --model_dir models/mtl_production \
  --epochs 50 \
  --batch_size 32 \
  --use_full_dataset
```

Consulta `CLAUDE.md` para m√°s detalles sobre el entrenamiento completo.
